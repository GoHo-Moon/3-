import streamlit as st
import pandas as pd
import os
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm  # í°íŠ¸ ë§¤ë‹ˆì €
from wordcloud import WordCloud
from konlpy.tag import Okt
from collections import Counter
import networkx as nx
from itertools import combinations
import seaborn as sns
import altair as alt
import plotly.express as px

import api  # ë„¤ì´ë²„ ë‰´ìŠ¤ API í˜¸ì¶œ í•¨ìˆ˜ (ë³„ë„ íŒŒì¼ì— êµ¬í˜„ ê°€ì •)

# ----------------------------------------------------------------------
# A. ì´ˆê¸° ì„¤ì • ë° í°íŠ¸ ì „ì—­ ë“±ë¡
# ----------------------------------------------------------------------

# í°íŠ¸ ê²½ë¡œ ì„¤ì • (WordCloudì™€ Matplotlib ëª¨ë‘ ì‚¬ìš©)

# LLMì„ ì‚¬ìš©,,, seaborn ë° networkxì—ì„œ font_pathë§Œìœ¼ë¡œëŠ” ì‹¤í–‰ì´ ì˜¤ë¥˜ê°€ ë‚¬ìŒ.
# LLM ì½”ë“œ ì°¸ì¡°
FONT_PATH = "./fonts/AppleSDGothicNeoB.ttf"
FONT_NAME = 'sans-serif' # ê¸°ë³¸ê°’
if os.path.exists(FONT_PATH):
    # 1. Matplotlibì— í°íŠ¸ ë“±ë¡
    try:
        fm.fontManager.addfont(FONT_PATH)
        # 2. ë“±ë¡ëœ í°íŠ¸ ì´ë¦„ ê°€ì ¸ì™€ì„œ ì„¤ì •
        FONT_NAME = fm.FontProperties(fname=FONT_PATH).get_name()
        plt.rc('font', family=FONT_NAME)
        plt.rc('axes', unicode_minus=False) # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€
    except Exception as e:
        # ë“±ë¡ ì‹¤íŒ¨ ì‹œ Windows ê¸°ë³¸ í°íŠ¸ ì‚¬ìš© 
        st.warning(f"í°íŠ¸ ë“±ë¡ ì˜¤ë¥˜: {e}. ê¸°ë³¸ í°íŠ¸ë¡œ ëŒ€ì²´ë©ë‹ˆë‹¤.")
        FONT_NAME = 'Malgun Gothic'
        plt.rc('font', family=FONT_NAME)
        plt.rc('axes', unicode_minus=False)
else:
    st.warning(f"í•œê¸€ í°íŠ¸ íŒŒì¼ì´ ì—†ì–´ ê¸°ë³¸ í°íŠ¸ë¡œ ì¶œë ¥ë©ë‹ˆë‹¤. ({FONT_PATH} í™•ì¸ í•„ìš”)")
    # íŒŒì¼ì´ ì—†ì„ ê²½ìš° Matplotlib ê¸°ë³¸ í°íŠ¸ ì„¤ì • ìœ ì§€

# ======================================================
# 1) í˜ì´ì§€ ì„¤ì • (
# ======================================================
st.set_page_config(
    page_title="ğŸ”¥KíŒ ë°ëª¬ í—Œí„°ìŠ¤ íŒ¬ë¤ í˜•ì„± í•µì‹¬ ìš”ì¸ ë¶„ì„ğŸ”¥",
    page_icon="ğŸ”",
    layout="wide"
)

# ======================================================
# 2) ìƒë‹¨ ê³ ì • í—¤ë”
# ======================================================
st.markdown(
    """
    <div style="padding:14px 18px; border-radius:10px; border:1px solid #ddd; background:#fafafa;">
        <div style="font-size:18px; font-weight:700;">
            C221082 ë¬¸í˜„ìœ¨
        </div>
        <div style="font-size:28px; font-weight:800; margin-top:6px;">
            ğŸ”¥KíŒ ë°ëª¬ í—Œí„°ìŠ¤ íŒ¬ë¤ í˜•ì„± í•µì‹¬ ìš”ì¸ ë¶„ì„ğŸ”¥
        </div>
    </div>
    """,
    unsafe_allow_html=True
)
st.write("")
st.info("ğŸ”¥KíŒ ë°ëª¬ í—Œí„°ìŠ¤ íŒ¬ë¤ í˜•ì„± í•µì‹¬ ìš”ì¸ ë¶„ì„ğŸ”¥")

# ----------------------------------------------------------------------
# B. [ìºì‹œ í•¨ìˆ˜] ë°ì´í„° ìˆ˜ì§‘ 
# ìºì‹œ í•¨ìˆ˜ë¥¼ í†µí•´ ê°™ì€ íŒŒë¼ë¯¸í„°ë¡œ ì—¬ëŸ¬ ë²ˆ í˜¸ì¶œ ì‹œ API í˜¸ì¶œì„ ì¤„ì„..!
# ----------------------------------------------------------------------
@st.cache_data
def fetch_news_data(keyword, num):
    return api.get_naver_news(keyword, num)

# ----------------------------------------------------------------------
# C. [ìºì‹œ í•¨ìˆ˜] ë¶ˆìš©ì–´ ë¡œë“œ (íŒŒì¼ IOëŠ” í•œ ë²ˆë§Œ)
# wordcloudì™€ networkx ë¶„ì„ ëª¨ë‘ì—ì„œ ë™ì¼í•œ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ ì‚¬ìš©..
# ë¯¸ë¦¬ ì •ì˜ í›„ ìºì‹± --> ìºì‹±ì„ í†µí•´ ì—¬ëŸ¬ ë²ˆ í˜¸ì¶œ ì‹œ ë¶€ë‹´ì„ ì¤„ì„.
# ê°•ì˜ì•ˆ ë³´ê³  ì‚¬ìš©.
# ----------------------------------------------------------------------
@st.cache_data
def get_stop_words(keyword):
    """ë¶ˆìš©ì–´ íŒŒì¼ì„ ì½ê³  ê²€ìƒ‰ì–´ ë° ê°•ì˜ì—ì„œ ì‚¬ìš©ëœ ë¶ˆìš©ì–´ë¥¼ ì¶”ê°€í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤."""
    stopwords_path = "./data/korean_stopwords.txt" # ê°•ì˜ë¡ì—ì„œ ê°€ì§€ê³ ì˜¨ ë¶ˆìš©ì–´
    stop_words = set()
    
    if os.path.exists(stopwords_path):
        with open(stopwords_path, "r", encoding="utf-8") as f:
            stop_words = set(line.strip() for line in f if line.strip())
    else:
        # ê¸°ë³¸ ë¶ˆìš©ì–´ (íŒŒì¼ ì—†ì„ ê²½ìš°.. ê·¸ëŸ´ ë¦¬ëŠ” ì—†ë‹¤.)
        stop_words = {"ê²ƒ", "ë“±", "ìœ„", "ìˆ˜", "ë°°", "ë§Œ", "ëª…", "ê´€ë ¨", "ëŒ€í•´", "ë‰´ìŠ¤", "ì†ë³´"}
        
    if keyword:
        stop_words.add(keyword)
        stop_words.add(keyword.replace(" ", ""))
    # ê°•ì˜ì•ˆì—ì„œ ì¶”ê°€ëœ ë¶ˆìš©ì–´ë“¤ (ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ setì— update)
    stop_words.update([
        "ì„œìš¸", "ì„œìš¸ì‹œ", "ë¶€ë™ì‚°", "ì£¼ìš”", "ì²«ì§¸", "ê²°ê³¼", "ì¡°ì‚¬", "ì•„í¬", "ëŒ€ë¹„", "ì¦ê¶Œ", 
        "ê°€ëŠ¥ì„±", "ëŒ€í‘œ", "ì‹œì ˆ", "ì œì", "ìµœê°•", "í™œìš©", "ìµœì§„", "íƒ€ìš´", "ìš”ì†Œ", "ì ìš©",
        "ì¤‘ì•™", "ì „ì£¼", "í•œêµ­", "í¬í•¨", "ë„ì‹œ", "ì¼ë¶€", "ì´ìŠˆ", "ë³´ê³ ì„œ", "ê°ˆë“±", "ë¯¸ë˜", 
        "ìœ„ì›", "í†µí•´", "ë¬¸ì œ", "NHíˆ¬ìì¦ê¶Œ", "ì•„ìœ ê²½ì œ_ë¶€ë™ì‚°", "quot", "ì¡°êµ­", 
        "ì¡°í¬ì—°", "ì‚¬ë©´", "ì‹¬ì¸µë¶„ì„", "ë…„", "ì›”", "ì¼", "ì‹œ" # ì‹œê°„ ê´€ë ¨ ë¶ˆìš©ì–´ ì¶”ê°€(ì‹œê³„ì—´ ë¶„ì„ì„ ìœ„í•´)
    ])
    return stop_words

# ----------------------------------------------------------------------
# D. [ìºì‹œ í•¨ìˆ˜] í†µí•© ë¶„ì„ (í˜•íƒœì†Œ ë¶„ì„ì€ í•œ ë²ˆë§Œ)
# ë§ˆì°¬ê°€ì§€ë¡œ wordcloudì™€ networkx ë¶„ì„ ëª¨ë‘ì—ì„œ ë™ì¼í•œ í˜•íƒœì†Œ ë¶„ì„ ê²°ê³¼ ì‚¬ìš©..
# LLMì˜ í˜ì„ ë¹Œë ¤, ìµœì í™”ë¥¼ ì§„í–‰í–ˆë‹¤. (95%ê·¸ëŒ€ë¡œ ì‚¬ìš©..)
# ----------------------------------------------------------------------
@st.cache_data
def analyze_data(df, keyword, min_len):
    """
    ë°ì´í„°í”„ë ˆì„ì„ ë¶„ì„í•˜ì—¬ ë‹¨ì–´ ë¹ˆë„(freq)ì™€ ì—£ì§€ ëª©ë¡(edge_list)ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    @st.cache_dataë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬´ê±°ìš´ í˜•íƒœì†Œ ë¶„ì„ì„ ìºì‹±í•©ë‹ˆë‹¤.
    """
    stop_words = get_stop_words(keyword)
    
    # í…ìŠ¤íŠ¸ í†µí•© (ì‹œë¦¬ì¦ˆë¡œ ì²˜ë¦¬)
    text_series = (df["title"].fillna("").astype(str) + " " + df["description"].fillna("").astype(str))
    
    okt = Okt()
    all_filtered_nouns = []
    all_edge_list = []
    
    # ê° ë¬¸ì„œë³„ë¡œ ì²˜ë¦¬
    for doc in text_series:
        # 1. ëª…ì‚¬ ì¶”ì¶œ
        nouns = okt.nouns(doc)
        
        # 2. í•„í„°ë§ (ìµœì†Œ ê¸¸ì´, ë¶ˆìš©ì–´)
        filtered_nouns = [n for n in nouns if len(n) >= min_len and n not in stop_words]
        
        # 3. ì›Œë“œí´ë¼ìš°ë“œìš©: ëª¨ë“  ë¬¸ì„œì˜ ëª…ì‚¬ë¥¼ í•©ì¹©ë‹ˆë‹¤.
        all_filtered_nouns.extend(filtered_nouns)
        
        # 4. ë„¤íŠ¸ì›Œí¬ìš©: ë™ì‹œ ë“±ì¥ ê´€ê³„ ìƒì„±
        # 4-1. ì¤‘ë³µ ì œê±°ëœ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ (ë„¤íŠ¸ì›Œí¬ ë¶„ì„ì—ì„  ë‹¨ì–´ì™€ì˜ ê´€ê³„ë¥¼ ë³´ê¸°ì— ì¤‘ë³µëœ ë‹¨ì–´ëŠ” setì„ í†µí•´ í•˜ë‚˜ë¡œ ì·¨ê¸‰.. LLMì˜ ì¡°ì–¸)
        unique_terms = sorted(set(filtered_nouns))
        # 4-2. ë‹¨ì–´ ìŒ ìƒì„± (ì¡°í•©)
        if len(unique_terms) >= 2: # ë‹¨ì–´ê°€ 2ê°œ ì´ìƒì¼ ë•Œë§Œ ì¡°í•© ìƒì„±
            all_edge_list.extend(combinations(unique_terms, 2)) # ì—£ì§€ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€


    # 5. ìµœì¢… ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
    freq = Counter(all_filtered_nouns)

    return freq, all_edge_list
# ----------------------------------------------------------------------
# E. [ìºì‹œ í•¨ìˆ˜] ì‹œê³„ì—´ ë¶„ì„ ë°ì´í„° ì „ì²˜ë¦¬
# plotly, seaborn, altair ì‹œê°í™”ì— ê³µí†µìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ì‹œê³„ì—´ ë°ì´í„° ì „ì²˜ë¦¬
# pubDateë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¼ë³„ ê¸°ì‚¬ ê±´ìˆ˜ ë° ìƒìœ„ Nê°œ í‚¤ì›Œë“œì˜ ì¼ë³„ ë“±ì¥ ë¹ˆë„ ê³„ì‚°..!

# ----------------------------------------------------------------------
@st.cache_data
def get_time_series_data(df, freq, min_len, top_n=5):
    """
    ì¼ë³„ ê¸°ì‚¬ ê±´ìˆ˜ ë° ìƒìœ„ Nê°œ í‚¤ì›Œë“œì˜ ì¼ë³„ ë“±ì¥ ë¹ˆë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    # pubDateë¥¼ ë‚ ì§œ(Date) í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ê³  ë‚ ì§œë§Œ ë‚¨ê¹ë‹ˆë‹¤.
    # pubDate ì¹¼ëŸ¼ì€ ì´ë¯¸ ë°ì´í„° ìˆ˜ì§‘ ë‹¨ê³„ì—ì„œ datetime ê°ì²´ë¡œ ë³€í™˜ë˜ì—ˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
    df_copy = df.copy() # ì›ë³¸ ë°ì´í„°í”„ë ˆì„ ë³´í˜¸
    df_copy['date'] = pd.to_datetime(df_copy['pubDate']).dt.date
    df_copy['datetime'] = pd.to_datetime(df_copy['pubDate']) # ìš”ì¼
    
    # 1. ì¼ë³„ ê¸°ì‚¬ ê±´ìˆ˜ (Plotlyìš©)
    daily_volume = df_copy.groupby('date').size().reset_index(name='ê¸°ì‚¬_ê±´ìˆ˜')
    
    # ìš”ì¼ ì»¬ëŸ¼ ì¶”ê°€ ë° í•œê¸€ ë³€í™˜
    daily_volume['datetime'] = pd.to_datetime(daily_volume['date'])
    daily_volume['ìš”ì¼'] = daily_volume['datetime'].dt.day_name(locale='ko_KR.utf-8')
    #ìš”ì¼ ì •ë³´ ì—…ë°ì´íŠ¸ -> llm ì°¸ê³ .
    # ìš”ì¼ ìˆœì„œë¥¼ ê°•ì œí•˜ì—¬ ê·¸ë˜í”„ ìˆœì„œê°€ ì›”, í™”, ìˆ˜... ìˆœìœ¼ë¡œ ë˜ë„ë¡ ì¹´í…Œê³ ë¦¬ ì„¤ì •
    day_order = ['ì›”ìš”ì¼', 'í™”ìš”ì¼', 'ìˆ˜ìš”ì¼', 'ëª©ìš”ì¼', 'ê¸ˆìš”ì¼', 'í† ìš”ì¼', 'ì¼ìš”ì¼']
    daily_volume['ìš”ì¼'] = pd.Categorical(daily_volume['ìš”ì¼'], categories=day_order, ordered=True)
    daily_volume = daily_volume.sort_values(['date']) # ë‚ ì§œ ìˆœ ì •ë ¬

    # 2. ìƒìœ„ Nê°œ í‚¤ì›Œë“œ ëª©ë¡ (Altair ì‹œê³„ì—´ ì¶”ì  ëŒ€ìƒ)
    top_words = [word for word, count in freq.most_common(top_n)]
    
    # 3. ì¼ë³„ í‚¤ì›Œë“œ ë¹ˆë„ (Altairìš©)
    all_time_series_data = []
    okt = Okt()
    
    # ë¶ˆìš©ì–´ ëª©ë¡ ë¡œë“œ (ìºì‹œ í•¨ìˆ˜ í˜¸ì¶œ)
    # min_len ë³€ìˆ˜ëŠ” analyze_dataì—ì„œ ì‚¬ìš©ë˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì§ì ‘ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    stop_words = get_stop_words(st.session_state.get("search_keyword", "")) 

    for date, group_df in df_copy.groupby('date'):
        # í•´ë‹¹ ì¼ìì˜ ëª¨ë“  í…ìŠ¤íŠ¸ ê²°í•©
        # (title + description)
        daily_text = " ".join(group_df["title"].fillna("").astype(str) + " " + group_df["description"].fillna("").astype(str))
        
        # í˜•íƒœì†Œ ë¶„ì„ ë° í•„í„°ë§
        nouns = okt.nouns(daily_text)
        # min_lenê³¼ stop_wordsë¥¼ ì ìš©í•˜ì—¬ í•„í„°ë§
        daily_nouns = [n for n in nouns if len(n) >= min_len and n not in stop_words] 
        daily_freq = Counter(daily_nouns)
        
        for word in top_words:
            all_time_series_data.append({
                'ë‚ ì§œ': date,
                'ë‹¨ì–´': word,
                'ë¹ˆë„': daily_freq.get(word, 0)
            })
    
    # [ìˆ˜ì •] for ë£¨í”„ ë°–ì—ì„œ DataFrameì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.
    time_series_df = pd.DataFrame(all_time_series_data) 
    
    return daily_volume, time_series_df.sort_values('ë‚ ì§œ')
# ======================================================
# 3) ì‚¬ì´ë“œë°” (ì¸í„°ë ‰í‹°ë¸Œí•œ ì¡°ì‘ êµ¬í˜„~)
# ======================================================
with st.sidebar:
    st.header("âš™ï¸ ë¶„ì„ ì„¤ì •")
    keyword = st.text_input(
        "ê²€ìƒ‰ì–´ ì…ë ¥",
        placeholder="ì˜ˆ: KíŒ ë°ëª¬ í—Œí„°ìŠ¤",
        key="search_keyword_input"
    )
    news_limit = st.slider(
        "ìˆ˜ì§‘ ê¸°ì‚¬ ìˆ˜", 
        min_value=100,
        max_value=1000,
        value=1000,
        step=100
    )
    st.subheader("ğŸ“Š ì‹œê°í™” ì˜µì…˜")
    wc_top_n = st.slider(
        "ì›Œë“œí´ë¼ìš°ë“œ ë‹¨ì–´ ìˆ˜ (Top N)",
        min_value=20,
        max_value=200,
        value=80,
        step=10
    )
    edge_top_n = st.slider(
        "ë„¤íŠ¸ì›Œí¬ ê´€ê³„ ìˆ˜ (Top N)",
        min_value=10,
        max_value=100,
        value=50,
        step=10
    )
    min_word_len = st.slider(
        "ë‹¨ì–´ ìµœì†Œ ê¸¸ì´",
        min_value=1,
        max_value=4,
        value=2
    )
    # [ì¶”ê°€] ì‹œê³„ì—´ Top N ì„¤ì •
    ts_top_n = st.slider(
        "ì‹œê³„ì—´ ë‹¨ì–´ ìˆ˜ (Top N)",
        min_value=1,
        max_value=10,
        value=5
    )
    search_btn = st.button("ìˆ˜ì§‘ ì‹œì‘", use_container_width=True)

# ======================================================
# 4) ë©”ì¸ í™”ë©´ â€“ ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰
# ======================================================
st.header("1. ë°ì´í„° ìˆ˜ì§‘")
if search_btn:
    if not keyword:
        st.warning("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    else:
        with st.spinner("ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ì¤‘..."):
            try:
                # ë°ì´í„° ìˆ˜ì§‘ (ìºì‹± í•¨ìˆ˜ ì‚¬ìš©)
                df = fetch_news_data(keyword, news_limit)
                
                if df is not None and not df.empty:
                    # st.session_stateì— ìˆ˜ì§‘ëœ ë°ì´í„°ì™€ ê²€ìƒ‰ì–´ ì €ì¥
                    st.session_state["news_df"] = df
                    st.session_state["search_keyword"] = keyword
                    st.success(f"ìˆ˜ì§‘ ì™„ë£Œ: {len(df)}ê±´")
                else:
                    st.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    st.session_state["news_df"] = pd.DataFrame() # ë¹ˆ DFë¡œ ì´ˆê¸°í™”
            except Exception as e:
                st.error(f"ì—ëŸ¬ ë°œìƒ: {e}")

# ======================================================
# 5) ë°ì´í„° í™•ì¸
# ======================================================
if "news_df" in st.session_state and not st.session_state["news_df"].empty:
    st.header("2. ìˆ˜ì§‘ ë°ì´í„° í™•ì¸")
    with st.expander("ë‰´ìŠ¤ ë°ì´í„° ë³´ê¸°"):
        st.dataframe(st.session_state["news_df"])

# ======================================================
# 6) í†µí•© ë¶„ì„ ì‹¤í–‰ ë° ë°ì´í„° ì¤€ë¹„
# ======================================================
if "news_df" in st.session_state and not st.session_state["news_df"].empty:
    df = st.session_state["news_df"]
    keyword = st.session_state.get("search_keyword", "")
    
    with st.spinner("í†µí•© í…ìŠ¤íŠ¸ ë¶„ì„ ì¤‘ (í˜•íƒœì†Œ ë¶„ì„ ë° ê´€ê³„ ìƒì„±)..."):
        # [í•µì‹¬] í†µí•© ë¶„ì„ í•¨ìˆ˜ í˜¸ì¶œ (ìºì‹œ í•¨ìˆ˜)
        try:
            freq, edge_list = analyze_data(df, keyword, min_word_len)
        except Exception as e:
            st.error(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            freq = Counter()
            edge_list = []

    if not freq:
        st.warning("ë¶„ì„ ê°€ëŠ¥í•œ ëª…ì‚¬ê°€ ì—†ì–´ ì›Œë“œí´ë¼ìš°ë“œ/ë„¤íŠ¸ì›Œí¬ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ë‹¨ì–´ ìµœì†Œ ê¸¸ì´ ì¡°ì ˆ í•„ìš”)")
    daily_volume, time_series_df = get_time_series_data(df, freq, min_word_len, ts_top_n)
# ======================================================
# 7) ì›Œë“œí´ë¼ìš°ë“œ ì‹œê°í™”
# ======================================================
st.header("3. ì›Œë“œí´ë¼ìš°ë“œ ì‹œê°í™”")
if "news_df" in st.session_state and not st.session_state["news_df"].empty and freq:
    
    # 1. ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
    wc = WordCloud(
        font_path=FONT_PATH if os.path.exists(FONT_PATH) else None,
        background_color="white",
        width=900,
        height=450,
        max_words=int(wc_top_n)
    ).generate_from_frequencies(
        dict(freq.most_common(int(wc_top_n)))
    )
    
    # 2. ì‹œê°í™” ì¶œë ¥
    fig = plt.figure(figsize=(12, 5))
    plt.imshow(wc, interpolation="bilinear")
    plt.axis("off")
    st.pyplot(fig, clear_figure=True)
    
    # 3. ë‹¨ì–´ ë¹ˆë„ í‘œ
    with st.expander("ë‹¨ì–´ ë¹ˆë„ Top 50"):
        st.dataframe(
            pd.DataFrame(
                freq.most_common(50),
                columns=["ë‹¨ì–´", "ë¹ˆë„"]
            ),
            use_container_width=True
        )

    st.info("""

            **ğŸ’¡ ì›Œë“œí´ë¼ìš°ë“œ í•´ì„ **
            * **ë…¸ë“œ(ë‹¨ì–´) í¬ê¸°**:  ë“±ì¥ ë¹ˆë„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤, KíŒ ë°ëª¬ í—Œí„°ìŠ¤ ë¶„ì„ë¥¼ ë¶„ì„ í•œ ê²½ìš°,
            ê°€ì¥ ë§ì´ ëœ¬ ë‹¨ì–´ëŠ” ì¼€ì´íŒ, ë°ëª¬, í—Œí„°ìŠ¤, ë„·í”Œë¦­ìŠ¤, ì¼€ë°í—Œ, ì• ë‹ˆë©”ì´ì…˜ ë“±ë“± ìœ¼ë¡œ
            ì´ë¥¼ í†µí•´ kíŒ ë°ëª¬ í—Œí„°ìŠ¤ëŠ” ë„·í”Œë¦­ìŠ¤ì™€ ê´€ë ¨ì´ ê¹Šë‹¤ëŠ” ê²ƒ, ì¼€ì´íŒì— ê´€ë ¨ëœ ì• ë‹ˆë¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """)
# ======================================================
# 8) ë„¤íŠ¸ì›Œí¬ ì‹œê°í™”
# ======================================================
st.header("4. í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„")

if "news_df" in st.session_state and not st.session_state["news_df"].empty and edge_list:

    # 1. ì—£ì§€ ë¹ˆë„ ê³„ì‚°
    edge_counts = Counter(edge_list)
    
    # 2. ìƒìœ„ Nê°œ ì—£ì§€ í•„í„°ë§
    top_edges = edge_counts.most_common(int(edge_top_n))

    if len(top_edges) == 0:
        st.warning(f"ìƒìœ„ {int(edge_top_n)}ê°œ ê´€ê³„ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ì„¤ì • ì¡°ì ˆ í•„ìš”)")
    else:
        # 3. ê·¸ë˜í”„ ê°ì²´ ìƒì„±
        G = nx.Graph()
        weighted_edges = [(u, v, weight) for (u, v), weight in top_edges]
        G.add_weighted_edges_from(weighted_edges)
        
        # 4. ì¤‘ì‹¬ì„± ê³„ì‚° (Degree Centrality: ë…¸ë“œ í¬ê¸° ê²°ì •)
        centrality = nx.degree_centrality(G)
        
        # 5. ì‹œê°í™” ì¤€ë¹„
        fig, ax = plt.subplots(figsize=(15, 15)) 
        
        # ë ˆì´ì•„ì›ƒ ê²°ì • (í˜ ê¸°ë°˜ ë°°ì¹˜)
        pos = nx.spring_layout(G, k=0.3, iterations=50, seed=42)
        
        # ë…¸ë“œ í¬ê¸° ë° ì—£ì§€ ë‘ê»˜ ì„¤ì •
        node_size = [v * 7000 for v in centrality.values()]
        edge_width = [d['weight'] * 0.05 for (u, v, d) in G.edges(data=True)]

        # 6. draw_networkx í˜¸ì¶œ
        nx.draw_networkx(
            G, 
            pos,
            with_labels=True,
            node_size=node_size,
            node_color="#63B8FF", # ì‚°ëœ»í•œ ìƒ‰ìƒìœ¼ë¡œ ë³€ê²½
            edge_color="gray",
            width=edge_width,
            font_family=FONT_NAME, # ì „ì—­ ì„¤ì •ëœ í°íŠ¸ ì‚¬ìš©
            font_size=12,
            alpha=0.8,
            ax=ax
        )
        
        ax.set_title(f"'{keyword}' í‚¤ì›Œë“œ ê´€ê³„ë§ (Top {int(edge_top_n)})", size=20)
        plt.axis('off')
        st.pyplot(fig, clear_figure=True)
        
        st.info(f"""
        **ğŸ’¡ ì‹œê°í™” í•´ì„ ê°€ì´ë“œ (ì´ {len(G.nodes())}ê°œ ë…¸ë“œ)**
        * [cite_start]**ë…¸ë“œ(ë‹¨ì–´) í¬ê¸°**: ì—°ê²° ì¤‘ì‹¬ì„±) , ì¦‰, ë‹¤ë¥¸ ë‹¨ì–´ë“¤ê³¼ ì–¼ë§ˆë‚˜ ë§ì´ ì§ì ‘ì ìœ¼ë¡œ ì—°ê²°ë˜ì—ˆëŠ”ì§€.
        * [cite_start]**ì„ (Edge) ë‘ê»˜**: ë™ì‹œ ë“±ì¥ ë¹ˆë„ (ê´€ê³„ì˜ ê°•ë„). ë‘ ë‹¨ì–´ê°€ ê¸°ì‚¬ì—ì„œ í•¨ê»˜ ë‚˜ì˜¨ íšŸìˆ˜ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.
        * [cite_start]**ë ˆì´ì•„ì›ƒ**: í˜ ê¸°ë°˜ ë°°ì¹˜. ê´€ê³„ê°€ ê°•í•œ ë‹¨ì–´ì¼ìˆ˜ë¡ ì„œë¡œ ê°€ê¹ê²Œ ë°°ì¹˜ë©ë‹ˆë‹¤.

        ê²°ë¡  : ê´€ê³„ê°€ ê¹Šì€ ìŒë“¤ì€ 'ì¼€ì´íŒ ë°ëª¬ í—Œí„°ìŠ¤'ë‚´ì˜ ë‹¨ì–´ë“¤ê³¼, ì• ë‹ˆë©”ì´ì…˜, ì¼€ì´íŒ, ì˜í™” ë“±ë“±ìœ¼ë¡œ
        'ì¼€ì´íŒ ë°ëª¬ í—Œí„°ìŠ¤'ê°€ ì¼€ì´íŒê³¼ ì• ë‹ˆë©”ì´ì…˜, ì˜í™” ë“±ê³¼ ê¹Šì€ ê´€ë ¨ì´ ìˆìŒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        """)
        
        # ìˆœì„œìŒìœ¼ë¡œ ì¡°ê¸ˆ ë” ì‰½ê²Œ ë³´ëŠ” ê¸°ëŠ¥ (ìˆ¨ê¸°ê¸° ê°€ëŠ¥=expander)
        with st.expander("ìƒìœ„ ê´€ê³„ ëª©ë¡"):
            st.dataframe(
                pd.DataFrame(
                    top_edges,
                    columns=["ë‹¨ì–´ ìŒ", "ë¹ˆë„"]
                ),
                use_container_width=True
            )

# ======================================================
# 9) Seaborn: ë¶„ì„ ê¸°ê°„ ì¤‘ ìƒìœ„ ë‹¨ì–´ ë¹ˆë„ ë§‰ëŒ€ ê·¸ë˜í”„
# ======================================================
st.header("5. ë‹¨ì–´ ë¹ˆë„ ë§‰ëŒ€ ê·¸ë˜í”„ (Seaborn)")
if "news_df" in st.session_state and not st.session_state["news_df"].empty and freq:
    
    top_n_freq_df = pd.DataFrame(
        freq.most_common(int(ts_top_n)), 
        columns=["ë‹¨ì–´", "ë¹ˆë„"]
    )

    if not top_n_freq_df.empty:
        fig, ax = plt.subplots(figsize=(10, 8))
        # ìˆ˜í‰ ë§‰ëŒ€ ê·¸ë˜í”„ (ë¹ˆë„ ìˆœ)
        sns.barplot(
            x="ë¹ˆë„", 
            y="ë‹¨ì–´", 
            data=top_n_freq_df.sort_values(by="ë¹ˆë„", ascending=False), 
            ax=ax,
            palette="viridis" # ìƒ‰ìƒ íŒ”ë ˆíŠ¸ ì§€ì •
        )
        ax.set_title(f"í‚¤ì›Œë“œ ë¹ˆë„ Top {int(ts_top_n)}", size=15)
        ax.set_xlabel("ë¹ˆë„", size=12)
        ax.set_ylabel("ë‹¨ì–´", size=12)
        plt.tight_layout()
        st.pyplot(fig, clear_figure=True)
    else:
        st.warning("ë§‰ëŒ€ ê·¸ë˜í”„ë¥¼ ìƒì„±í•  ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    st.info(f"""
        **ğŸ’¡ ì‹œê°í™” í•´ì„ ê°€ì´ë“œ 
             * **ë§‰ëŒ€ ê·¸ë˜í”„**: ê° ë‹¨ì–´ì˜ ë¹ˆë„ë¥¼ ì‹œê°ì ìœ¼ë¡œ ë¹„êµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
             * **ìƒ‰ìƒ**: ë‹¨ì–´ì˜ ì¤‘ìš”ë„ë¥¼ ë‚˜íƒ€ë‚´ë©°, ë” ì§„í•œ ìƒ‰ì¼ìˆ˜ë¡ ë†’ì€ ë¹ˆë„ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.
             * **ë ˆì´ì•„ì›ƒ**: ìˆ˜í‰ ë§‰ëŒ€ ê·¸ë˜í”„ëŠ” ë‹¨ì–´ì˜ ìƒëŒ€ì ì¸ ë¹ˆë„ë¥¼ ì‰½ê²Œ íŒŒì•…í•  ìˆ˜ ìˆê²Œ ë„ì™€ì¤ë‹ˆë‹¤.
            
            top 10ìœ¼ë¡œ í–‡ì„ ë•ŒëŠ” ë°ëª¬, í—Œí„°ìŠ¤ ,ì¼€ì´íŒ,,,, ë„·í”Œë¦­ìŠ¤ ì• ë‹ˆë©”ì´ì…˜, ì˜í™”, ì˜¬í•´, ì¸ê¸°, ë¯¸êµ­'
            ìœ¼ë¡œ ì¼€ì´íŒ ë°ëª¬ í—Œí„°ìŠ¤ëŠ” ì˜¬í•´ ì¸ê¸° ìˆëŠ” ì˜í™”/ì• ë‹ˆ/ë„·í”Œë¦­ìŠ¤ë¡œì„œ íŠ¹íˆ ë¯¸êµ­ì—ë„ ì¸ê¸°ê°€ ë§ë‹¤ëŠ” ê²ƒì„ ìœ ì¶”í•  ìˆ˜ ìˆë‹¤.
            """)
# ======================================================
# 10) Plotly: ì¼ë³„ ë‰´ìŠ¤ ë°œí–‰ ê±´ìˆ˜ ì‹œê³„ì—´ ê·¸ë˜í”„
# ======================================================
st.header("6. ì¼ë³„ ë‰´ìŠ¤ ë°œí–‰ëŸ‰ ì¶”ì´ (Plotly)")
if "news_df" in st.session_state and not st.session_state["news_df"].empty:
    
    if not daily_volume.empty:
        # Plotly Expressë¥¼ ì´ìš©í•œ ì‹œê³„ì—´ êº¾ì€ì„  ê·¸ë˜í”„ (ì¶”ì„¸ë¥¼ ë³´ê¸° ì¢‹ìœ¼ë‹ˆ)
        fig = px.line(
            daily_volume,
            x='date',
            y='ê¸°ì‚¬_ê±´ìˆ˜',
            title='ì¼ë³„ ê¸°ì‚¬ ë°œí–‰ ê±´ìˆ˜ ë³€í™” ì¶”ì´',
            labels={'date': 'ë‚ ì§œ', 'ê¸°ì‚¬_ê±´ìˆ˜': 'ê¸°ì‚¬ ê±´ìˆ˜'},
            line_shape='linear',  # êº¾ì€ì„  ê·¸ë˜í”„.
            markers=True,         # ê° ë°ì´í„° í¬ì¸íŠ¸ì— ë§ˆì»¤(ì ) í‘œì‹œ
            color_discrete_sequence=['#1F77B4'] # íŒŒë€ìƒ‰ ê³„ì—´ë¡œ ë³€ê²½ (ì„  ê·¸ë˜í”„ì— ì¼ë°˜ì ) # ìƒ‰ì€ llmí•œí…Œ ë¬¼ì–´ë´„
        )
        
        # ë ˆì´ì•„ì›ƒ ì—…ë°ì´íŠ¸ (ì‹œê³„ì—´ ìµœì í™”)
        fig.update_xaxes(
            tickformat="%Y-%m-%d", 
            title='ë‚ ì§œ'
        )
        fig.update_layout(
            xaxis_title='ë‚ ì§œ', 
            yaxis_title='ê¸°ì‚¬ ê±´ìˆ˜',
            hovermode="x unified" # ë§ˆìš°ìŠ¤ ì˜¤ë²„ ì‹œ xì¶• ê¸°ì¤€ìœ¼ë¡œ í†µí•© íˆ´íŒ í‘œì‹œ
        )
        
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.warning("ì¼ë³„ ë‰´ìŠ¤ ë°œí–‰ ê±´ìˆ˜ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    st.info("""
            **ğŸ’¡ ì‹œê°í™” í•´ì„ ê°€ì´ë“œ**)
            **êº¾ì€ì„  ê·¸ë˜í”„**: ì‹œê°„ì— ë”°ë¥¸ ë‰´ìŠ¤ ë°œí–‰ëŸ‰ì˜ ë³€í™”ë¥¼ ì‹œê°ì ìœ¼ë¡œ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            **ì¶”ì„¸ ë¶„ì„**: íŠ¹ì • ê¸°ê°„ ë™ì•ˆ ë‰´ìŠ¤ ë°œí–‰ëŸ‰ì´ ì¦ê°€í•˜ê±°ë‚˜ ê°ì†Œí•˜ëŠ” ê²½í–¥ì„ ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            **ì´ë²¤íŠ¸ ì—°ê´€ì„±**: ë‰´ìŠ¤ ë°œí–‰ëŸ‰ì˜ ê¸‰ì¦ì´ íŠ¹ì • ì´ë²¤íŠ¸ë‚˜ ì´ìŠˆì™€ ì—°ê´€ë˜ì–´ ìˆëŠ”ì§€ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            í˜„ì¬ ì£¼ì–´ì§„ ë°ì´í„°ë¡œëŠ” ì–´ë–¤ ë„·í”Œë¦­ìŠ¤ì— ì–´ë–¤ ì´ìŠˆê°€ ë°œìƒí•˜ì˜€ëŠ”ì§€ ë“±ì€ ì•Œ ìˆ˜ ì—†ì§€ë§Œ, íŠ¹ì • ì´ë²¤íŠ¸ì— ëŒ€í•œ ì •ë³´ë¥¼ ì–»ëŠ”ë‹¤ë©´
            ë³¸ ì°¨íŠ¸ë¥¼ í†µí•´ ì¼ë³„ë¡œ ë°œí–‰ëŸ‰ì„ ì •í™•í•˜ê³  ì§ê´€ì ìœ¼ë¡œ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                """)
# ======================================================
# 11) Altair: ìƒìœ„ ë‹¨ì–´ì˜ ì¼ë³„ ë“±ì¥ ë¹ˆë„ ì¶”ì´
# ======================================================
st.header(f"7. ìƒìœ„ {ts_top_n}ê°œ í‚¤ì›Œë“œ ì¼ë³„ ë“±ì¥ ì¶”ì´ (Altair)")
if "news_df" in st.session_state and not st.session_state["news_df"].empty and not time_series_df.empty:

    # Altair ì°¨íŠ¸ ìƒì„±
    chart = alt.Chart(time_series_df).mark_line().encode(
        # xì¶•: ë‚ ì§œ (ì‹œê³„ì—´)
        x=alt.X('ë‚ ì§œ:T', title='ë‚ ì§œ'),
        # yì¶•: ë¹ˆë„ (ì •ëŸ‰ì  ë°ì´í„°)
        y=alt.Y('ë¹ˆë„:Q', title='ë¹ˆë„'),
        # ìƒ‰ìƒ: ë‹¨ì–´ë³„ êµ¬ë¶„
        color=alt.Color('ë‹¨ì–´:N'),
        # íˆ´íŒ: ë§ˆìš°ìŠ¤ ì˜¤ë²„ ì‹œ ìƒì„¸ ì •ë³´ í‘œì‹œ
        tooltip=['ë‚ ì§œ:T', 'ë‹¨ì–´:N', 'ë¹ˆë„:Q']
    ).properties(
        title=f"ì¼ë³„ ìƒìœ„ {ts_top_n}ê°œ í‚¤ì›Œë“œ ë“±ì¥ ë¹ˆë„ ë³€í™”"
    ).interactive() # ì¤Œ/íŒ¨ë‹ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •
    
    st.altair_chart(chart, use_container_width=True)
else:
    st.warning(f"ìƒìœ„ {ts_top_n}ê°œ í‚¤ì›Œë“œ ì¼ë³„ ë“±ì¥ ì¶”ì´ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„ í™•ì¸ í•„ìš”)")
st.info("""
         **ğŸ’¡ ì‹œê°í™” í•´ì„ ê°€ì´ë“œ**
         **ì„  ê·¸ë˜í”„**: ê° í‚¤ì›Œë“œì˜ ì¼ë³„ ë“±ì¥ ë¹ˆë„ ë³€í™”ë¥¼ ì‹œê°ì ìœ¼ë¡œ ë¹„êµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
         **ìƒ‰ìƒ**: ê° í‚¤ì›Œë“œëŠ” ê³ ìœ í•œ ìƒ‰ìƒìœ¼ë¡œ êµ¬ë¶„ë˜ì–´ ìˆì–´, íŠ¹ì • í‚¤ì›Œë“œì˜ ì¶”ì´ë¥¼ ì‰½ê²Œ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
         **íˆ´íŒ**: ë§ˆìš°ìŠ¤ ì˜¤ë²„ ì‹œ í•´ë‹¹ ë‚ ì§œì™€ í‚¤ì›Œë“œì˜ ì •í™•í•œ ë¹ˆë„ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
         ê²°ë¡  : í‚¤ì›Œë“œë“¤ ëª¨ë‘ ë™ì¼í•œ ì¶”ì„¸ë¡œ ì¦ê°€í•˜ê³  ê°ì†Œí•˜ëŠ” í­ì„ ë³´ì´ê³  ìˆë‹¤.
         ì´ëŠ” ìƒìœ„ 10ê°œ í‚¤ì›Œë“œì˜ ì¶”ì´ê°€ ë¹„ìŠ·í•˜ë‹¤, ì„œë¡œ ìƒê´€ì´ ìˆë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•  ìˆ˜ ìˆë‹¤.
         """)
